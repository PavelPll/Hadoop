(1)PREPARE cluster
sudo reboot all instances
ssh master
stop-all.sh
start-dfs.sh
start-yarn.sh

(2)START DATABASE
ssh datadode3
nohup /home/ubuntu/db-derby/bin/startNetworkServer -h 0.0.0.0 &
CHECK it on 1527
nmap -p 1527 172.31.13.177
1527/tcp open  tlisrv

(3)ADD DB FILE to HDFS
wget https://datasets.imdbws.com/name.basics.tsv.gz
hdfs dfs -put ./name.basics.tsv.gz /name.basics.tsv.gz
hdfs fsck /name.basics.tsv.gz | grep "Total blocks"
Connecting to namenode via http://masternode:9870/fsck?ugi=ubuntu&path=%2Fname.basics.tsv.gz
 Total blocks (validated):	2 (avg. block size 126729535 B)

(4)START HIVE
hive
CREATE DATABASE name_db;
USE name_db;

(5)add data to DB:
CREATE TABLE t5
(
    nconst VARCHAR(64),
    primaryName string,
    birthYear INT,
    deathYear INT,
    primaryProfession  string,
    knownForTitles  string
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY "\t"
LINES TERMINATED BY "\n"
STORED AS TEXTFILE;
LOAD DATA INPATH '/name.basics.tsv.gz'
INTO TABLE t5;

(6)SELECT COUNT(*) FROM t5;
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 15.96 sec   HDFS Read: 253472554 HDFS Write: 108 SUCCESS
Total MapReduce CPU Time Spent: 15 seconds 960 msec
OK
12831412
Time taken: 51.396 seconds, Fetched: 1 row(s)

(7)It looks like/probably hadoop is not capable run parallel jobs (map>1) for .gz files.
Another reason could be low numbers of hdfs blocks (only two in our case).
Another check seems also be ok:
SET hive.exec.mode.local.auto=false;
SET mapred.map.tasks; gives 2
SET mapred.map.tasks=20;
Lets try another thing: convert the initial data file from .gz to .csv and repeat the procedure
to see whether it increases the number of mappers.
INSERT OVERWRITE LOCAL DIRECTORY '/home/ubuntu/data/csv.csv'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
SELECT * FROM T5;
-----------------
hdfs dfs -put ./csv.csv /csv.csv
hdfs fsck /csv.csv | grep "Total blocks"
Connecting to namenode via http://masternode:9870/fsck?ugi=ubuntu&path=%2Fcsv.csv
 Total blocks (validated):	6 (avg. block size 130139146 B)
------------------
DROP TABLE T5;
CREATE TABLE t5
(
    nconst VARCHAR(64),
    primaryName string,
    birthYear INT,
    deathYear INT,
    primaryProfession  string,
    knownForTitles  string
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY "\t"
LINES TERMINATED BY "\n"
STORED AS TEXTFILE;
------------------------
LOAD DATA INPATH '/csv.csv'
INTO TABLE t5;
------------------------
hive> SELECT * FROM t5 LIMIT 5;
OK
nconst	primaryName	NULL	NULL	primaryProfession	knownForTitles
nm0000001	Fred Astaire	1899	1987	soundtrack,actor,miscellaneous	tt0045537,tt0050419,tt0053137,tt0072308
nm0000002	Lauren Bacall	1924	2014	actress,soundtrack	tt0075213,tt0117057,tt0038355,tt0037382
nm0000003	Brigitte Bardot	1934	NULL	actress,soundtrack,music_department	tt0057345,tt0056404,tt0049189,tt0054452
nm0000004	John Belushi	1949	1982	actor,soundtrack,writer	tt0077975,tt0078723,tt0080455,tt0072562
Time taken: 0.334 seconds, Fetched: 5 row(s)
-------------------------------
SELECT COUNT(*) FROM t5;
Hadoop job information for Stage-1: number of mappers: 3; number of reducers: 1
2023-09-11 13:21:41,809 Stage-1 map = 0%,  reduce = 0%
2023-09-11 13:22:41,888 Stage-1 map = 0%,  reduce = 0%
2023-09-11 13:23:42,484 Stage-1 map = 0%,  reduce = 0%
2023-09-11 13:24:20,963 Stage-1 map = 33%,  reduce = 0%, Cumulative CPU 5.26 sec
2023-09-11 13:24:23,044 Stage-1 map = 67%,  reduce = 0%, Cumulative CPU 9.34 sec
2023-09-11 13:24:29,288 Stage-1 map = 78%,  reduce = 0%, Cumulative CPU 15.33 sec
2023-09-11 13:24:37,657 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 14.39 sec
2023-09-11 13:24:38,718 Stage-1 map = 100%,  reduce = 22%, Cumulative CPU 14.65 sec
2023-09-11 13:24:44,946 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 17.45 sec
2023-09-11 13:25:45,626 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 17.69 sec
MapReduce Total cumulative CPU time: 17 seconds 690 msec
Ended Job = job_1694435874634_0005
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 3  Reduce: 1   Cumulative CPU: 17.69 sec   HDFS Read: 780882781 HDFS Write: 108 SUCCESS
Total MapReduce CPU Time Spent: 17 seconds 690 msec
OK
12831412

(8) Yes, using .csv instead of .gz files increases the number of mappers from 1 to 3.
We see in (7) that for csv.csv we get 3 mappers (see (7)) while for .gz (see (6)) only 1 mapper. 

(9) LETS try select statement for 100000 and for 200000 lines in data:
hive> CREATE TABLE T5_1
    > AS
    > SELECT 
    > nconst,
    > primaryName,
    > birthYear,
    > deathYear,
    > sort_array(SPLIT(primaryProfession, ",")) primaryProfession, 
    > sort_array(SPLIT(knownForTitles, ",")) knownForTitles
    > FROM t5 LIMIT 100000;
Hadoop job information for Stage-1: number of mappers: 3; number of reducers: 1
2023-09-11 15:20:41,332 Stage-1 map = 0%,  reduce = 0%
2023-09-11 15:21:41,871 Stage-1 map = 0%,  reduce = 0%, Cumulative CPU 165.48 sec
2023-09-11 15:22:42,218 Stage-1 map = 0%,  reduce = 0%, Cumulative CPU 342.82 sec
2023-09-11 15:22:48,448 Stage-1 map = 33%,  reduce = 0%, Cumulative CPU 367.45 sec
2023-09-11 15:23:05,060 Stage-1 map = 33%,  reduce = 11%, Cumulative CPU 395.13 sec
2023-09-11 15:23:48,075 Stage-1 map = 67%,  reduce = 11%, Cumulative CPU 460.01 sec
2023-09-11 15:23:53,185 Stage-1 map = 67%,  reduce = 22%, Cumulative CPU 468.27 sec
2023-09-11 15:24:53,667 Stage-1 map = 67%,  reduce = 22%, Cumulative CPU 527.15 sec
2023-09-11 15:25:54,127 Stage-1 map = 67%,  reduce = 22%, Cumulative CPU 586.13 sec
2023-09-11 15:26:54,649 Stage-1 map = 67%,  reduce = 22%, Cumulative CPU 645.12 sec
2023-09-11 15:27:24,381 Stage-1 map = 100%,  reduce = 22%, Cumulative CPU 677.05 sec
2023-09-11 15:27:27,518 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 681.76 sec
MapReduce Total cumulative CPU time: 11 minutes 21 seconds 760 msec
Ended Job = job_1694444675591_0002
Moving data to directory hdfs://masternode:9000/user/hive/warehouse/name_db.db/t5_1
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 3  Reduce: 1   Cumulative CPU: 681.76 sec   HDFS Read: 20283672 HDFS Write: 5486150 SUCCESS
Total MapReduce CPU Time Spent: 11 minutes 21 seconds 760 msec
OK
Time taken: 424.314 seconds
-------------------------------------------------
hive> CREATE TABLE T5_1
    > AS
    > SELECT 
    > nconst,
    > primaryName,
    > birthYear,
    > deathYear,
    > sort_array(SPLIT(primaryProfession, ",")) primaryProfession, 
    > sort_array(SPLIT(knownForTitles, ",")) knownForTitles
    > FROM t5 LIMIT 200000;
Query ID = ubuntu_20230911152815_be6a02fe-8383-4893-9625-9ae940fc49b1
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1694444675591_0003, Tracking URL = http://masternode:8088/proxy/application_1694444675591_0003/
Kill Command = /home/ubuntu/hadoop/bin/mapred job  -kill job_1694444675591_0003
Hadoop job information for Stage-1: number of mappers: 3; number of reducers: 1
2023-09-11 15:28:27,449 Stage-1 map = 0%,  reduce = 0%
2023-09-11 15:29:28,406 Stage-1 map = 0%,  reduce = 0%, Cumulative CPU 112.06 sec
2023-09-11 15:32:51,111 Stage-1 map = 0%,  reduce = 0%, Cumulative CPU 124.6 sec
2023-09-11 15:33:51,472 Stage-1 map = 0%,  reduce = 0%, Cumulative CPU 706.36 sec
2023-09-11 15:34:51,716 Stage-1 map = 0%,  reduce = 0%, Cumulative CPU 873.79 sec
2023-09-11 15:35:51,821 Stage-1 map = 0%,  reduce = 0%, Cumulative CPU 1053.19 sec
2023-09-11 15:36:51,948 Stage-1 map = 0%,  reduce = 0%, Cumulative CPU 1232.83 sec
2023-09-11 15:37:52,029 Stage-1 map = 0%,  reduce = 0%, Cumulative CPU 1401.1 sec
2023-09-11 15:38:53,058 Stage-1 map = 0%,  reduce = 0%, Cumulative CPU 1599.58 sec
2023-09-11 15:38:59,343 Stage-1 map = 33%,  reduce = 0%, Cumulative CPU 1623.7 sec
2023-09-11 15:39:24,247 Stage-1 map = 33%,  reduce = 11%, Cumulative CPU 1673.84 sec
2023-09-11 15:40:24,517 Stage-1 map = 33%,  reduce = 11%, Cumulative CPU 1801.09 sec
2023-09-11 15:41:24,947 Stage-1 map = 33%,  reduce = 11%, Cumulative CPU 1927.58 sec
2023-09-11 15:42:25,171 Stage-1 map = 33%,  reduce = 11%, Cumulative CPU 2054.25 sec
2023-09-11 15:43:25,342 Stage-1 map = 33%,  reduce = 11%, Cumulative CPU 2181.3 sec
2023-09-11 15:44:25,637 Stage-1 map = 33%,  reduce = 11%, Cumulative CPU 2301.99 sec
2023-09-11 15:45:25,690 Stage-1 map = 33%,  reduce = 11%, Cumulative CPU 2429.73 sec
2023-09-11 15:46:25,844 Stage-1 map = 33%,  reduce = 11%, Cumulative CPU 2550.71 sec
2023-09-11 15:47:10,407 Stage-1 map = 67%,  reduce = 11%, Cumulative CPU 2660.89 sec
2023-09-11 15:47:14,498 Stage-1 map = 67%,  reduce = 22%, Cumulative CPU 2660.96 sec
2023-09-11 15:48:15,013 Stage-1 map = 67%,  reduce = 22%, Cumulative CPU 2725.39 sec
2023-09-11 15:48:39,570 Stage-1 map = 100%,  reduce = 22%, Cumulative CPU 2761.62 sec
2023-09-11 15:48:44,691 Stage-1 map = 100%,  reduce = 67%, Cumulative CPU 2765.33 sec
2023-09-11 15:48:46,735 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2766.06 sec
MapReduce Total cumulative CPU time: 46 minutes 6 seconds 60 msec
Ended Job = job_1694444675591_0003
Moving data to directory hdfs://masternode:9000/user/hive/warehouse/name_db.db/t5_1
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 3  Reduce: 1   Cumulative CPU: 2766.06 sec   HDFS Read: 39711000 HDFS Write: 10955551 SUCCESS
Total MapReduce CPU Time Spent: 46 minutes 6 seconds 60 msec
OK
Time taken: 1233.06 seconds
------------------------------------
nm13883781	Abi Jo Hollins	NULL	NULL	["actor"]	["tt21378162"]
nm13883780	Joseph Macken	NULL	NULL	["actor"]	["tt21378162"]
nm1388377	Pascal Baillargeau	NULL	NULL	["actor","camera_department","cinematographer"]	["tt0329485","tt0424525","tt1234108","tt4161544"]
nm13883779	Kurorchester Bad Windsheim	NULL	NULL	[]	["tt21378146"]
nm13883778	Original-Zigeuner-Ensemble	NULL	NULL	[]	["tt21378146"]

(10) LETS do buckets:
CREATE TABLE T5_buckets
(
    nconst VARCHAR(64),
    primaryName string,
    birthYear INT,
    deathYear INT,
    primaryProfession  string,
    knownForTitles  string
)
CLUSTERED BY (birthYear) INTO 16 BUCKETS
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ","
LINES TERMINATED BY "\n"
STORED AS TEXTFILE;
-------------------------------------------
INSERT INTO TABLE T5_buckets
SELECT  nconst, primaryName, birthYear, deathYear, primaryProfession, knownForTitles
FROM T5 LIMIT 200000;
Query ID = ubuntu_20230912172236_5fcf5a0b-face-44b5-b2a8-f1b54b5ab08f
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1694539098097_0001, Tracking URL = http://masternode:8088/proxy/application_1694539098097_0001/
Kill Command = /home/ubuntu/hadoop/bin/mapred job  -kill job_1694539098097_0001
Hadoop job information for Stage-1: number of mappers: 3; number of reducers: 1
2023-09-12 17:22:55,841 Stage-1 map = 0%,  reduce = 0%
2023-09-12 17:23:05,706 Stage-1 map = 33%,  reduce = 0%, Cumulative CPU 6.74 sec
2023-09-12 17:23:14,483 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 19.2 sec
2023-09-12 17:23:19,752 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 26.33 sec
MapReduce Total cumulative CPU time: 26 seconds 330 msec
Ended Job = job_1694539098097_0001
Launching Job 2 out of 3
Number of reduce tasks determined at compile time: 16
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1694539098097_0002, Tracking URL = http://masternode:8088/proxy/application_1694539098097_0002/
Kill Command = /home/ubuntu/hadoop/bin/mapred job  -kill job_1694539098097_0002
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 16
2023-09-12 17:23:37,319 Stage-2 map = 0%,  reduce = 0%
2023-09-12 17:23:48,738 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 7.02 sec
2023-09-12 17:24:34,971 Stage-2 map = 100%,  reduce = 4%, Cumulative CPU 7.02 sec
2023-09-12 17:26:06,009 Stage-2 map = 100%,  reduce = 58%, Cumulative CPU 37.74 sec
2023-09-12 17:27:14,621 Stage-2 map = 100%,  reduce = 67%, Cumulative CPU 41.17 sec
2023-09-12 17:29:08,159 Stage-2 map = 100%,  reduce = 71%, Cumulative CPU 45.89 sec
2023-09-12 17:30:25,068 Stage-2 map = 100%,  reduce = 75%, Cumulative CPU 52.83 sec
2023-09-12 17:32:04,336 Stage-2 map = 100%,  reduce = 75%, Cumulative CPU 52.83 sec
2023-09-12 17:34:14,223 Stage-2 map = 100%,  reduce = 81%, Cumulative CPU 56.37 sec
2023-09-12 17:34:37,188 Stage-2 map = 100%,  reduce = 88%, Cumulative CPU 58.81 sec
2023-09-12 17:36:40,304 Stage-2 map = 100%,  reduce = 88%, Cumulative CPU 58.81 sec
2023-09-12 17:38:47,253 Stage-2 map = 100%,  reduce = 90%, Cumulative CPU 58.81 sec
2023-09-12 17:39:12,609 Stage-2 map = 100%,  reduce = 94%, Cumulative CPU 62.45 sec
2023-09-12 17:40:33,883 Stage-2 map = 100%,  reduce = 94%, Cumulative CPU 62.45 sec
2023-09-12 17:41:50,253 Stage-2 map = 100%,  reduce = 94%, Cumulative CPU 62.45 sec
2023-09-12 17:42:54,171 Stage-2 map = 100%,  reduce = 94%, Cumulative CPU 62.45 sec
2023-09-12 17:44:14,114 Stage-2 map = 100%,  reduce = 94%, Cumulative CPU 62.45 sec
2023-09-12 17:45:40,473 Stage-2 map = 100%,  reduce = 94%, Cumulative CPU 62.45 sec
2023-09-12 17:47:14,149 Stage-2 map = 100%,  reduce = 94%, Cumulative CPU 62.45 sec
2023-09-12 17:48:49,177 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 62.99 sec
2023-09-12 17:51:55,717 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 66.22 sec
2023-09-12 17:53:52,890 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 66.22 sec
2023-09-12 17:56:38,567 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 66.22 sec
2023-09-12 18:02:52,889 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 66.22 sec
MapReduce Total cumulative CPU time: 1 minutes 6 seconds 220 msec
Ended Job = job_1694539098097_0002
Loading data to table name_db.t5_buckets
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1694539098097_0003, Tracking URL = http://masternode:8088/proxy/application_1694539098097_0003/
Kill Command = /home/ubuntu/hadoop/bin/mapred job  -kill job_1694539098097_0003
Hadoop job information for Stage-4: number of mappers: 2; number of reducers: 1
2023-09-12 18:03:18,287 Stage-4 map = 0%,  reduce = 0%
2023-09-12 18:03:32,978 Stage-4 map = 50%,  reduce = 0%, Cumulative CPU 1.86 sec
2023-09-12 18:03:34,010 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 4.02 sec
2023-09-12 18:03:44,393 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 6.29 sec
MapReduce Total cumulative CPU time: 6 seconds 290 msec
Ended Job = job_1694539098097_0003
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 3  Reduce: 1   Cumulative CPU: 26.33 sec   HDFS Read: 39706545 HDFS Write: 13042092 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 16   Cumulative CPU: 66.22 sec   HDFS Read: 13185647 HDFS Write: 10999317 SUCCESS
Stage-Stage-4: Map: 2  Reduce: 1   Cumulative CPU: 6.29 sec   HDFS Read: 66828 HDFS Write: 4765 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 38 seconds 840 msec
OK
Time taken: 2471.046 seconds


